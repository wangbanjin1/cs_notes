# 操作系统

#### 栈帧问题（以xv6为例）

我们需要关注的有: **stack**栈是向上增长的, 即每次新call一个函数, 就会新allocate一个**stack frame**在当前stack frame的头上(更高地址address处). 当一个函数完成后, 它的stack frame就会被销毁, 同时**program counte**r就重新跳转到上一个函数跳进来的位置的下一条指令. 这个跳转的位置是可以从**Return Address**这个位置读出来的.

那么对于我们的**backtrace**函数, 在有了当前**frame pointe**r(fp)后, 我们只需要不断地print出**Return Address**(地址为fp-0x8), 然后把**fp**设置成上一个函数的**fp**(地址为fp-0x10)。

<img src="https://pic1.zhimg.com/80/v2-33c7d5eae1f1577562cfb9e89ebdc750_720w.webp" alt="img" style="zoom:50%;" />

当被调用函数执行完毕并准备返回时，首先会执行返回指令，该指令会将程序控制权转移到返回地址所指示的位置，即调用函数的下一条指令。这个过程确实导致程序回到了调用函数的栈帧，但并不会立即销毁当前函数的栈帧。

然后，程序会继续执行调用函数的指令，直到调用函数也执行完毕。当调用函数执行完毕后，会执行一系列清理操作，包括销毁当前函数的栈帧。这时候，帧指针会更新为上一个函数的帧指针，指向调用函数的栈底。

所以，要总结这个过程：

> 1. 被调用函数执行完毕后，首先会跳转到返回地址，这导致程序回到调用函数的栈帧，开始执行调用函数的指令。
> 2. 调用函数执行完毕后，当前函数的栈帧会被销毁，并将帧指针更新为上一个函数的帧指针，指向调用函数的栈底。

#### 进程调度算法

进程调度是从进程的就绪队列中，按照一定的算法，选择一个进程并将CPU分配给它运行，以实现进程的并发执行。进程的调度算法有如下几种：

 1、**先来先服务调度算法**。先来先服务调度算法按照进程到达的先后顺序进行调度，先到的进程就先被调度，然后进程会一直运行。直到线程退出或被阻塞，才会选择下一个进程接着运行。当一个长作业先运行了，那么后面的短作业等待的时间就会很长。 

2、**最短作业优先调度算法**。每次调度时选择当前已到达的且运行时间最短的进程。如果一直有短作业到来，那么长作业永远得不到调度。 

3、**高响应比优先调度算法**。高响应比优先调度算法每次进行进程调度时，先计算响应比优先级，为响应比最高的进程分配 CPU。 

4、**最短剩余时间优先算法**。最短剩余时间优先算法按剩余运行时间的顺序进行调度，当一个新的进程到达时，把它所需要的整个运行时间与当前进程的剩余运行时间作比较。如果新的进程需要的时间更少，则挂起当前进程，运行新的进程，否则新的进程等待。

 5、**时间片轮转调度算法**。时间片轮转调度算法每个进程被分配一个时间片，允许进程在该时间片内运行。如果时间片用完，进程还在运行，那么将会把此进程从CP 释放出来，并把 CPU分配给另外一个进程。 

6、**最高优先级调度算法**。最高优先级调度算法为每个进程分配一个优先级，从就绪队列中选择最高优先级的进程进行运行。为了防止低优先级的进程永远等不到调度，可以随着时间的推移增加等待进程的优先级。

 7、**多级反馈队列调度算法**。多级反馈队列调度算法有多个队列，每个队列优先级从高到低，同时优先级越高时间片越短。如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列。

#### 中断和异常

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240723100325726.png" alt="image-20240723100325726" style="zoom:50%;" />

中断是指 CPU 对系统发生某事件时的一种响应，即CPU 暂停正在执行的程序，在保留现场后，自动地转去执行该事件的中断处理程序，执行完后，再返回到原程序的断点处继续执行。 

中断分为外中断和内中断。 外中断，就是我们指的中断，是指由于外部设备事件所引起的中断，如常见的磁盘中断、打印机中断等；中断由外因引起，与现行指令无关，是正在运行的程序所不期望的，中断的引入是为了支持CPU和设备之间的并行操作。 

内中断，就是异常，是指由于 CPU 内部事件所引起的中断，比如程序的非法指令或者地址越界。异常是由CPU本身原因引起，表示CPU执行指令时本身出现的问题。 中断会使 CPU 由用户态变为内核态，使操作系统重新夺回对 CPU 的控制权，中断是让操作系统内核夺回 CPU 使用权的唯一途径。如果没有中断机制，那么一旦应用程序上 CPU 运行，CPU 就会一直运行这个应用程序。 不同的中断信号，需要不同的中断处理程序来处理。当 CPU 检测到中断信号后，会根据中断信号的类型去查询“中断向量表”，以此来找到相应的中断处理程序在内存中的存放位置。

#### 缓存IO 直接IO

缓存I/O（Buffered IO）又被称作标准I/O，大多数文件系统的默认I/O操作都是缓存I/O。

在Linux的缓存I/O机制中，数据先从磁盘复制到内核空间的缓冲区，然后从内核空间缓冲区复制到应用程序的地址空间。 读操作时，先检查内核的缓冲区有没有需要的数据，如果已经缓存了，那么就直接从缓存中返回；否则从磁盘中读取，然后缓存在内核的缓存中。 写操作时，将数据从用户空间复制到内核空间的缓存中，这时对用户程序来说写操作就已经完成，至于什么时候再写到磁盘中由操作系统决定，除非显示地调用了sync同步命令。 直接IO(Direct IO)是应用程序直接读写文件，而不经过内核缓冲区，也就是绕过内核缓冲区，自己管理IO缓存区，这样做的目的是减少一次内核缓冲区到用户程序缓存的数据复制。 缓存IO和直接IO都是通过内核的文件系统读写文件，而裸IO(Raw IO)是绕过文件系统，直接对磁盘磁盘数据进行读写。 应用场景上，缓存IO通过将文件数据缓存到内核缓冲区中，可以减少磁盘读写的次数，从而提高性能，缓存IO适用于频繁读写小文件的场景。直接IO绕开了内核缓冲区，减少了操作系统缓冲区和用户地址空间的拷贝次数，适用于不需要频繁读写的大文件操作。裸IO绕过文件系统，直接读写磁盘块设备数据，一般在数据库中用得比较多。

#### 死锁问题

死锁，是导致线程卡死的锁冲突，简单来说就是两个或者两个以上的线程在执行的过程中，争夺同一个共享资源造成的相互等待的现象。 产生死锁有四个必要条件，同时满足这四个条件，死锁才会发生。 

1、互斥条件 互斥条件是指多个线程不能同时使用同一个资源，若一个线程已经拥有了该资源，那么其他想获取该资源的线程就需要阻塞等待。

 2、不可剥夺条件 不可剥夺条件是指当一个资源被线程获取了之后，如果该线程不主动释放该资源，那么该资源一直被占有，其他想获取该资源的线程就要一直进行等待。

 3、请求与保持条件 请求与保持条件是指线程已经拥有了一个资源，但又提出了新的资源请求，而该资源已被其他线程占有，此时请求线程被阻塞，但对自己已获得的资源保持不放。 

4、循环等待条件。 循环等待条件是指在死锁发生的时候，两个线程获取资源的顺序构成了环形链，环路中每一个线程所占有的资源同时被另一个线程申请，也就是前一个线程占有后一个线程所申请的资源。 要避免死锁问题，一般方法是打破循环等待条件，让两个线程之间获取资源的顺序不要穿插在一起，可以让一个线程先获得两个资源，使用完毕后释放，另一个线程再获取这两个资源，就可以保证两个线程都能正常执行。

#### 进程、线程状态

进程一般分五个状态：创建，就绪，运行，阻塞，结束
线程一般分四个状态：就绪，运行，阻塞，死亡

为了方便对各个进程的管理，操作系统一般将进程划分为运行状态、就绪状态、阻塞状态、创建状态和结束状态五种状态。

其中运行状态、就绪状态和阻塞状态这三种是进程的基本状态，需要我们重点关注。
运行状态表示进程占有着CPU并正在运行。

就绪状态表示进程已分配到所需资源，已经具备运行条件，但是由于此时没有空闲的CPU而暂时不能运行。

阻塞状态表示进程正在等待某一事件（比如IO请求）的发生而暂时停止运行，处于阻塞状态时，进程本身不具备运行条件，即使给它分配CPU资源也无法运行。

进程的另外两种状态，创建态表示进程正在被创建，尚未转到就绪态。结束状态表示进程正从系统中消失，可能是进程正常结束或其他原因退出运行。


当进程的运行环境在满足一定条件后，就会从一种状态变化为另外一种状态。进程间的状态变化总共有以下六种情况：
1、当进程被创建完成并初始化后，一切就绪准备运行时，此时进程就从创建状态变为就绪状态。

2、处于就绪状态的进程被操作系统的进程调度器选中后，就分配给 CPU 正式运行该进程，此时进程就从就绪状态变为运行状态。

3、当进程已经运行完成或出错时，会被操作系统作结束状态处理，此时进程就从运行状态变为结束状态。

4、处于运行状态的进程在运行过程中，由于分配给它的运行时间片用完，操作系统会把该进程变为就绪态，接着从就绪态选中另外一个进程运行，此时进程就从运行态变为就绪态。

5、处于运行状态的进程请求某个事件，例如请求 I/O 事件，必须等待时，此时就从就从运行状态变为阻塞状态。
6、当进程等待的事件完成时，此时进程就从阻塞状态变为就绪状态。

进程状态变化需要注意两点:


1、进程只能从运行状态变为阻塞状态，无法从就绪状态变为阻塞状态，因为进程变为阻塞状态是进程去请求某种资源导致的，是进程自发的行为，必然是发生在进程正在运行处于运行状态的时候。

2、进程无法直接从阻塞状态变为运行状态，因为一个进程被分配CPU资源进行运行，是由操作系统进行调度的，进程请求到资源后，只能先变为就绪状态，然后等待操作系统的调度，等被分配到CPU资源后，才会变为运行状态。

#### 线程间的同步方式

线程同步的实现方式主要有6种：互斥锁、自旋锁、读写锁、条件变量、屏障、信号量。 

1、互斥锁。互斥锁在访问共享资源前对互斥量进行加锁，在访问完成后释放互斥量进行解锁。对互斥量加锁以后，任何其他试图再次对互斥量加锁的线程都会被阻塞，直至当前线程释放该互斥量。 

2、自旋锁。自旋锁与互斥量类似，但它不使线程进入阻塞态，而是在获取锁之前一直占用CPU，处于忙等自旋状态。自旋锁适用于锁被持有的时间短且线程不希望在重新调度上花费太多成本的情况。 

互斥锁得阻塞和唤醒操作会涉及到上下文切换，因此在系统上的开销比较大，自旋锁避免了上下文切换，在锁持有持有时间较短的情况下性能比较好，但是长时间自选等待会浪费cpu资源。

3、读写锁。读写锁有三种状态：读模式加锁、写模式加锁和不加锁，一次只有一个线程可以占有写模式的读写锁，但是多个线程可以同时占有读模式的读写锁。读写锁非常适合对数据结构读的次数远大于写的情况。 

4、条件变量。条件变量允许线程睡眠，直到满足某种条件，当满足条件时，可以向该线程发送信号，通知并唤醒该线程。条件变量通常与互斥量配合一起使用。条件变量由互斥量保护，线程在改变条件状态之前必须首先锁住互斥量，其他线程在获得互斥量之前不会察觉到条件的改变，因为必须在锁住互斥量之后它才可以计算条件是否发生变化。 

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812090631394.png" alt="image-20240812090631394" style="zoom:67%;" />

当多个线程需要等待某个特定条件成立才能继续执行时，条件变量就显得尤为重要。通过条件变量，线程可以安全地进入等待状态，直到被其他线程显式地唤醒或满足等待的条件。这有助于避免线程的无谓轮询或忙等待，提高了系统的响应能力和效率。

5、屏障。屏障是用户协调多个线程并行工作的同步机制。屏障允许每个线程等待，直到所有的合作线程都到达某一点，然后从该点继续执行。 

6、信号量。信号量本质上是一个计数器，用于为多个进程提供共享数据对象的访问。编程时可根据操作信号量值的结果判断是否对公共资源具有访问的权限，当信号量值大于 0 时，则可以访问，否则将阻塞。PV 原语是对信号量的操作，一次 P 操作使信号量减１，一次 V 操作使信号量加１。

应用场景上， 互斥锁适用于临界区资源访问时间较长或存在阻塞操作的情况 自旋锁适用于临界区资源访问时间短，且线程竞争不激烈的情况 读写锁适用于读操作远远多于写操作的场景，可以提高并发读性能。

#### 孤儿进程和僵尸进程

孤儿进程：孤儿进程指的是父进程死掉后还在执行自己任务的子进程。一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。 

僵尸进程：一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中，这种进程称之为僵尸进程。 孤儿进程由于init进程有善后作用，所以孤儿进程没有什么危害，但是僵尸进程虽然运行实体已经消失，但是仍然在内核的进程表中占据一条记录，这样长期下去会造成系统资源的浪费。 

对于僵尸进程的处理，有如下几种办法： 1、通过信号机制：子进程退出时向父进程发送SIGCHILD信号，父进程处理SIGCHILD信号，调用wait()或者waitpid()，让父进程阻塞等待僵尸进程的出现，处理完再继续运行父进程。 2、杀死父进程：当父进程陷入死循环等无法处理僵尸进程时，强制杀死父进程，那么它的子进程，即僵尸进程会变成孤儿进程，由init进程来回收。 3、重启系统：当系统重启时，所有进程在系统关闭时被停止，包括僵尸进程，开启时init进程会重新加载其他进程。

#### 进程的调度算法

进程调度是从进程的就绪队列中，按照一定的算法，选择一个进程并将CPU分配给它运行，以实现进程的并发执行。进程的调度算法有如下几种：

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812102801391.png" alt="image-20240812102801391" style="zoom:50%;" />

1、先来先服务调度算法。先来先服务调度算法按照进程到达的先后顺序进行调度，先到的进程就先被调度，然后进程会一直运行。直到线程退出或被阻塞，才会选择下一个进程接着运行。当一个长作业先运行了，那么后面的短作业等待的时间就会很长。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812102825774.png" alt="image-20240812102825774" style="zoom:50%;" />

2、最短作业优先调度算法。最短作业优先调度算法每次调度时选择当前已到达的且运行时间最短的进程。如果一直有短作业到来，那么长作业永远得不到调

度。

3、高响应比优先调度算法。高响应比优先调度算法每次进行进程调度时，先计算响应比优先级，为响应比最高的进程分配 CPU。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812102925533.png" alt="image-20240812102925533" style="zoom:50%;" />

4、最短剩余时间优先算法。最短剩余时间优先算法按剩余运行时间的顺序进行调度，当一个新的进程到达时，把它所需要的整个运行时间与当前进程的剩余运行时间作比较。如果新的进程需要的时间更少，则挂起当前进程，运行新的进程，否则新的进程等待。

5、时间片轮转调度算法。时间片轮转调度算法每个进程被分配一个时间片，允许进程在该时间片内运行。如果时间片用完，进程还在运行，那么将会把此进程从CPU 释放出来，并把 CPU分配给另外一个进程。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812103011151.png" alt="image-20240812103011151" style="zoom:50%;" />

6、最高优先级调度算法。最高优先级调度算法为每个进程分配一个优先级，从就绪队列中选择最高优先级的进程进行运行。为了防止低优先级的进程永远等不到调度，可以随着时间的推移增加等待进程的优先级。

7、多级反馈队列调度算法。多级反馈队列调度算法有多个队列，每个队列优先级从高到低，同时优先级越高时间片越短。如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列。

#### 中断和异常的区别

中断是指 CPU 对系统发生某事件时的一种响应，即CPU 暂停正在执行的程序，在保留现场后，自动地转去执行该事件的中断处理程序，执行完后，再返回到原程序的断点处继续执行。

 中断分为外中断和内中断。 外中断，就是我们指的中断，是指由于外部设备事件所引起的中断，如常见的磁盘中断、打印机中断等；中断由外因引起，与现行指令无关，是正在运行的程序所不期望的，

中断的引入是为了支持CPU和设备之间的并行操作。 

内中断，就是异常，是指由于 CPU 内部事件所引起的中断，比如程序的非法指令或者地址越界。异常是由CPU本身原因引起，表示CPU执行指令时本身出现的问题。 中断会使 CPU 由用户态变为内核态，使操作系统重新夺回对 CPU 的控制权，中断是让操作系统内核夺回 CPU 使用权的唯一途径。如果没有中断机制，那么一旦应用程序上 CPU 运行，CPU 就会一直运行这个应用程序。 不同的中断信号，需要不同的中断处理程序来处理。当 CPU 检测到中断信号后，会根据中断信号的类型去查询“中断向量表”，以此来找到相应的中断处理程序在内存中的存放位置。 

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812103304903.png" alt="image-20240812103304903" style="zoom:50%;" />

#### 死锁产生

死锁，是导致线程卡死的锁冲突，简单来说就是两个或者两个以上的线程在执行的过程中，争夺同一个共享资源造成的相互等待的现象。 产生死锁有四个必要条件，同时满足这四个条件，死锁才会发生。

 1、互斥条件 互斥条件是指多个线程不能同时使用同一个资源，若一个线程已经拥有了该资源，那么其他想获取该资源的线程就需要阻塞等待。

 2、不可剥夺条件 不可剥夺条件是指当一个资源被线程获取了之后，如果该线程不主动释放该资源，那么该资源一直被占有，其他想获取该资源的线程就要一直进行等待。

 3、请求与保持条件 请求与保持条件是指线程已经拥有了一个资源，但又提出了新的资源请求，而该资源已被其他线程占有，此时请求线程被阻塞，但对自己已获得的资源保持不放。

 4、循环等待条件。 循环等待条件是指在死锁发生的时候，两个线程获取资源的顺序构成了环形链，环路中每一个线程所占有的资源同时被另一个线程申请，也就是前一个线程占有后一个线程所申请的资源。 要避免死锁问题，一般方法是打破循环等待条件，让两个线程之间获取资源的顺序不要穿插在一起，可以让一个线程先获得两个资源，使用完毕后释放，另一个线程再获取这两个资源，就可以保证两个线程都能正常执行。

#### 缓存IO 直接IO 裸IO

缓存I/O（Buffered IO）又被称作标准I/O，大多数文件系统的默认I/O操作都是缓存I/O。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812105205050.png" alt="image-20240812105205050" style="zoom:50%;" />

在Linux的缓存I/O机制中，数据先从磁盘复制到内核空间的缓冲区，然后从内核空间缓冲区复制到应用程序的地址空间。 读操作时，先检查内核的缓冲区有没有需要的数据，如果已经缓存了，那么就直接从缓存中返回；否则从磁盘中读取，然后缓存在内核的缓存中。 写操作时，将数据从用户空间复制到内核空间的缓存中，这时对用户程序来说写操作就已经完成，至于什么时候再写到磁盘中由操作系统决定，除非显示地调用了sync同步命令。 

直接IO(Direct IO)是应用程序直接读写文件，而不经过内核缓冲区，也就是绕过内核缓冲区，自己管理IO缓存区，这样做的目的是减少一次内核缓冲区到用户程序缓存的数据复制。 

缓存IO和直接IO都是通过内核的文件系统读写文件，而裸IO(Raw IO)是绕过文件系统，直接对磁盘磁盘数据进行读写。 应用场景上，缓存IO通过将文件数据缓存到内核缓冲区中，可以减少磁盘读写的次数，从而提高性能，缓存IO适用于频繁读写小文件的场景。直接IO绕开了内核缓冲区，减少了操作系统缓冲区和用户地址空间的拷贝次数，适用于不需要频繁读写的大文件操作。裸IO绕过文件系统，直接读写磁盘块设备数据，一般在数据库中用得比较多。

#### 阻塞，非阻塞，同步和异步IO区别

IO读取数据分为两个阶段，第一个阶段是内核准备好数据，第二个阶段是内核把数据从内核态拷贝到用户态。 阻塞IO是当用户调用 read 后，用户线程会被阻塞，等内核数据准备好并且数据从内核缓冲区拷贝到用户态缓存区后， read 才会返回。阻塞IO是两个阶段都会阻塞，没有数据时也会阻塞。 

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812105730698.png" alt="image-20240812105730698" style="zoom:50%;" />

非阻塞IO是调用read后，如果没有数据就立马返回，通过不断轮询的方式去调用read，直到数据被拷贝到用户态的应用程序缓冲区，read请求才获取到结果。非阻塞IO阻塞的是第二个阶段，第一阶段没有数据时不会阻塞，第二阶段等待内核把数据从内核态拷贝到用户态的过程中才会阻塞。 

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812105756776.png" alt="image-20240812105756776" style="zoom:50%;" />

同步 IO是应用程序发起一个 IO 操作后，必须等待内核把 IO 操作处理完成后才返回。无论 read 是阻塞 I/O，还是非阻塞 I/O， 都是同步调用，因为在 read 调用时，第二阶段内核将数据从内核空间拷贝到用户空间的过程都是需要等待的。 

异步 IO应用程序发起一个 IO 操作后，调用者不能立刻得到结果，而是在内核完成 IO 操作后，通过信号或回调来通知调用者。异步 I/O 是内核数据准备好和数据从内核态拷贝到用户态这两个过程都不用等待。 

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812105834212.png" alt="image-20240812105834212" style="zoom:50%;" />

总结一下，只有同步才有阻塞和非阻塞之分，异步必定是非阻塞的。 只有用户线程在操作IO的时候根本不去考虑IO的执行，全部都交给内核去完成， 而自己只等待一个完成信号的时候，才是真正的异步IO。select、poll、epool等IO多路复用方式都是同步的。

#### 进程，线程和协程得区别

进程是资源分配的最小单位，每个进程都有自己的独立内存空间，进程由进程控制块、程序段和数据段组成。 进程控制块（PCB）保存进程运行期间相关的数据，是进程存在的唯一标志。 程序段是能被进程调度程序调度到CPU 运行的程序的代码段。 数据段用来存储程序运行期间的相关数据。 进程是应用程序运行的载体，可看做是正在执行的程序。程序本身是没有生命周期的，只是存在于磁盘上的一些指令集合，但程序一旦被运行起来就是进程。启动后的进程，会依赖操作系统的调度完成生命周期的转换。由于每个进程都有独立的代码和数据空间，所以进程间的切换会有较大的开销。 

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812110901899.png" alt="image-20240812110901899" style="zoom:50%;" />

线程是CPU任务调度和执行的最小单位，一个进程中可以包含多个线程，这些线程可以并发运行，且共享进程提供的相同的代码和数据空间，每个线程都有自己独立的运行栈和程序计数器，线程间切换的开销小。

 协程，又称微线程，是一种用户态的轻量级线程，一个线程可以有多个协程。协程的调度完全由用户控制（也就是在用户态执行）。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到线程的堆区，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以协程的上下文切换非常快。 线程在进程的内部，它不能独立执行，必须依存于进程中，由进程提供多个线程的执行控制，进程和线程都可以并发的执行。 协程抽象于线程之上，线程是被分割的CPU资源, 协程是组织好的代码流程, 协程需要线程来承载运行。与线程相比，协程占用资源小，由用户调度，切换开销小，同一个线程中的协程不需要使用锁，执行效率更高。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812111030007.png" alt="image-20240812111030007" style="zoom:50%;" />

#### 并发和并行的区别

并发是指两个或多个任务在同一时间间隔内发生。当有多个线程在操作时,如果系统只有一个CPU,则它不可能真正同时执行一个以上的线程，它只能把CPU运行时间划分成若干个时间段,再将时间段分配给各个线程执行，在一个时间段的线程代码运行时，其它线程处于挂起等待状态。这些线程微观上都是被顺序执行的，只是由于每个线程分配的时间段特别短，线程执行的切换速度非常快，在宏观上看起来他们像是被同时执行的。 

并行是指两个或者多个任务在同一时间点发生。当系统有一个以上CPU时,则线程的操作有可能会并行。当一个CPU执行一个线程时，另一个CPU可以执行另一个线程，两个线程互不抢占CPU资源，可以同时进行。并行是真正的同时执行。

#### 进程间通信

进程间通信的方式有管道、消息队列、共享内存、信号量、信号和套接字。

 管道分为匿名管道和命名管道，它是一种半双工的通信方式，数据只能单向流动，管道的通信数据遵循先进先出的原则；匿名管道只能用在父子进程之间传输数据，命名管道可以在不相关进程间通信；管道的通信效率低，不适合进程间频繁地交换数据。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812144249799.png" alt="image-20240812144249799" style="zoom:50%;" />

 消息队列是保存在内核中的消息链表，按照消息的类型进行消息传递，具有较高的可靠性和稳定性。消息队列的消息体有一个最大长度的限制，所以不适合比较大的数据的传输。消息队列通信过程中，存在用户态与内核态之间的数据拷贝开销。 

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812144359046.png" alt="image-20240812144359046" style="zoom:50%;" />

共享内存是映射一段能被其他进程所访问的内存，这段内存由一个进程创建，但多个进程都可以访问；共享内存不需要陷入内核态或者系统调用，大大提高了通信的速度，是最快的进程间通信方式，但是当多进程竞争同一个共享资源时，使用共享内存会造成数据错乱的问题。 

信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，与共享内存结合起来使用，用来实现进程和线程对临界区的同步及互斥访问。 

信号是一种异步通信机制，用于通知接收进程某个事件已经发生，比如使用kill命令，就是给进程发信号。 

套接字不仅可以用于不同主机间的进程间通信，也可以用于本地主机上的进程间通信。

这里简单提一下信号。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812145934510.png" alt="image-20240812145934510" style="zoom:50%;" />

#### IO多路复用

select、poll和epoll都是用于实现 I/O 多路复用的方式，可以在同一时间内监听多个文件描述符的就绪状态。 select 是一种比较老的方式，它使用位图来表示文件描述符的状态。调用 select 时，内核需要遍历整个位图，检查每个文件描述符是否就绪。这种轮询的方式在连接数量很少时还是很有效的，但当连接数量增多时，性能会下降。由于使用位图来保存描述符，所以 select 还有描述符个数的限制，一般只能支持 2048 个，不过 select 的跨平台性比较好，几乎所有的平台都可以支持。 

poll 使用链表结构来表示文件描述符的状态，没有最大连接数的限制。和 select 函数一样，poll 返回后，需要轮询来获取就绪的描述符，因此随着监视的描述符数量的增长，其效率也会线性下降。 

epoll 是 Linux 特有的一种方式，它使用了事件驱动的模型，没有最大连接数的限制。它将文件描述符添加到 epoll 的事件集合中，等待事件的发生。与 select 和 poll 不同的是，epoll 不需要轮询，它使用回调的方式，只关注真正发生事件的文件描述符。这使得epoll在大规模高并发连接下具有卓越的性能。

#### 零拷贝

零拷贝是指计算机执行IO操作时，CPU不需要将数据从一个存储区域复制到另一个存储区域，从而减少上下文切换以及数据拷贝的时间。零拷贝是一种IO操作优化技术，它并不是真的没有数据拷贝，而是广义上讲的减少和避免不必要的数据拷贝。以读取一个文件并通过socket发送文件数据为例，一般需要read和write两个系统调用，这种传统的IO读写流程，需要4次用户态和内核态的上下文切换和4次数据拷贝，这些额外的上下文切换和数据拷贝，会消耗大量的CPU资源和内存带宽，降低数据传输的效率。零拷贝减少了上下文切换和内存拷贝的次数，主要实现方式有四种：

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812151150374.png" alt="image-20240812151150374" style="zoom:50%;" />

 1、mmap read系统调用会把内核缓冲区的数据拷贝到用户缓冲区里，为了减少这一开销，我们可以用 mmap 替换 read 。mmap直接把内核缓冲区里的数据映射到用户空间，这样内核与用户空间就不需要再进行任何的数据拷贝操作。mmap+write方式发生了四次上下文切换以及三次数据拷贝，相对传统的IO方式，减少了一次CPU数据拷贝。 

2、sendfile sendfile是一个专门发送文件的系统调用函数，适用于将数据从文件拷贝到 socket 套接字上，使用sendfile需要硬件的支持。通过sendfile传输文件，只需要两次上下文切换和两次DMA数据拷贝，相对传统的IO方式，sendfile去除了CPU拷贝，提升了系统性能。

3、splice splice不需要硬件支持，可以在内核空间的读缓冲区和网络缓冲区之间建立管道，从而避免了两者之间的 CPU 拷贝操作。splice可以用于源文件描述符和目的文件描述符都是socket的情况。 splice使用了 Linux 的管道缓冲机制，可以在任意两个文件描述符间传输数据，但是两个文件描述符中必须有一个是管道。

4、tee tee与splice类似，但两个文件描述符都必须是管道。tee是两个描述符之间进行数据复制，不会消耗数据，因此源文件描述符上的数据仍然可以用于后续的读操作。

#### 页面置换算法

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812151514301.png" alt="image-20240812151514301" style="zoom:50%;" />

进程运行时，若其访问的页面不在内存， 便会产生一个缺页中断，请求操作系统将所缺页调入到物理内存，但如果这时内存已无空闲空间，就需要从内存中调出一页程序或数据，送入磁盘的对换区。选择调出页面的算法就称为页面置换算法。

好的页面置换算法应有较低的页面更换频率，也就是说，应将以后不会再访问或者以后较长时间内不会再访问的页面先调出。

常见的置换算法有以下五种： 

1 最佳置换算法：最佳置换算法所选择的被淘汰页面将是以后永不使用的，或者是在最长时间内不再被访问的页面，这样可以保证获得最低的缺页率。但由于人们无法提前预知进程在内存的若干页面中哪个是未来最长时间内不再被访问的，因而该算法无法实现。

2 先进先出置换算法：先进先出置换算法优先淘汰最早进入内存的页面，即在内存中驻留时间最久的页面。该算法实现简单，只需把调入内存的页面根据先后次序链接成队列，设置一个指针总指向最早的页面。但该算法与进程实际运行时的规律不适应，因为在进程中，有的页面经常被访问。

3 最近最久未使用置换算法：最近最久未使用置换算法选择最近最长时间未访问过的页面予以淘汰，它认为过去一段时间内未访问过的页面，在最近的将来可能也不会被访问。该算法为每个页面设置一个访问字段，来记录页面自上次被访问以来所经历的时间，淘汰页面时选择现有页面中值最大的予以淘汰。该算法性能较好，但需要寄存器和栈的硬件支持。

4 时钟置换算法：时钟置换算法把所有的页面都保存在一个类似钟面的环形链表中，一个表针指向最老的页面，当发生缺页中断时，算法首先检查表针指向的页面，如果它的访问位位是 0 就淘汰该页面，并把新的页面插入这个位置，然后把表针前移一个位置。如果访问位是 1 就清除访问位，并把表针前移一个位置，重复这个过程直到找到了一个访问位为 0 的页面为止。

 5 最不常用置换算法：最不常用置换算法对每个页面设置一个访问计数器，每当一个页面被访问时，该页面的访问计数器就累加 1。在发生缺页中断时，淘汰计数器值最小也就是访问次数最少的那个页面。最不常用置换算法只考虑了访问频率问题，没考虑时间的问题，比如有些页面在过去时间里访问的频率很高，但是现在已经没有访问了，而当前频繁访问的页面由于没有这些页面访问的次数高，在发生缺页中断时，就会可能会误伤当前刚开始频繁访问，但访问次数还不高的页面。

#### 磁盘调度算法

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812151844450.png" alt="image-20240812151844450" style="zoom:50%;" />

在计算机系统中，各个进程可能会不断提出对磁盘进行读写操作的请求。由于这些进程的发送请求的速度可能比磁盘响应的速度还要快，因此，我们有必要为每个磁盘设备建立一个等待队列，磁盘调度算法通过优化磁盘的访问请求顺序来提高磁盘的访问性能。寻道的时间是磁盘访问最耗时的部分，如果请求顺序优化的得当，必然可以节省一些不必要的寻道时间，从而提高磁盘的访问性能。 常见的磁盘调度算法有以下六种：

 1 先来先服务算法    先来先服务算法根据进程请求访问磁盘的先后顺序进行调度，先到来的请求先被服务。先来先服务算法中每个进程请求都能依次得到处理，不会出现某一进程的请求长期得不到满足，但是如果大量进程竞争使用磁盘，请求访问的磁道可能会很分散，在性能上就会显得很差，所以先来先服务算法适用于磁盘I/O进程数目较少的场合。

 2  最短寻找时间优先算法    最短寻找时间优先算法每次优先选择从当前磁头位置所需寻道时间最短的请求。这种算法性能较好，但是可能产生饥饿现象，因为磁盘请求是一个动态的过程，可能会有距离磁头位置较近的请求不断过来，导致磁头在一小块区域来回移动，而距离磁头位置较远的请求一直得不到调度。

 3  扫描算法    扫描算法是磁头在一个方向上移动，访问所有未完成的请求，直到磁头到达该方向上的最后的磁道，才调换方向。扫描算法也叫电梯算法，就好像电梯保持一个方向移动，直到在那个方向上没有请求为止，然后再改变方向。扫描算法性能较好，不会产生饥饿现象，但是每个磁道的响应频率不平均，因为它只有到达最内侧或者最外侧才可以改变磁头移动方向，导致中间部分相比其他部分响应的频率会比较多。

 4  循环扫描算法    循环扫描算法只有磁头朝某个特定方向移动时才处理磁道访问请求，而返回时直接快速移动至起始端而不处理任何请求。循环扫描算法使得每个磁道的响应频率基本一致，但是只有到达最边上的磁道时才会改变方向，效率还不够高。

 5  LOOK算法    扫描算法中磁头只有移动到最内侧或者最外侧才能改变磁头方向，事实上有的时候最外侧已经没有请求了，这样就会造成时间的浪费。LOOK算法对扫描算法进行了优化，如果在某个方向上已经没有了请求，那么磁头可以直接改变方向。 

 6 C-LOOK 算法    循环扫描算法到达最边上的磁道时才能改变磁头方向，并且磁头返回时需要返回到最边缘的磁道上，C-LOOK算法对循环扫描算法进行了优化，如果磁头移动方向上已经没有请求时，就立即让磁头返回，并且磁头只需要返回到有磁盘访问请求的位置即可。

#### 内存碎片

内存碎片即“碎片的内存”，它分为外部碎片和内部碎片。外部碎片指的是还没有被分配出去（不属于任何进程），但由于太小了无法分配给申请内存空间的新进程的内存空闲区域。内部碎片就是已经被分配出去（能明确指出属于哪个进程）却不能被利用的内存空间。

 操作系统为了防止多进程运行时造成的内存地址冲突，引入了虚拟内存地址，为每个进程提供了一个独立的虚拟内存空间，使得进程以为自己独占全部内存资源。操作系统使用分段和分页的机制管理虚拟地址与物理地址的映射关系，正是由于操作系统采用的这些内存管理机制导致了内存碎片的产生。 

内存分段机制，简单理解就是根据程序申请使用内存的需要，来把物理内存分成一段一段来管理，比如程序需要100M的内存，分段机制就给1段100M连续空间的物理内存与之对应。内存分段可以做到根据实际需求分配内存，有多少需求就分配多大的段，所以不会出现内部碎片。 但是由于每个段的长度不固定，多个段未必能恰好使用所有的内存空间，所以会产生多个不连续的小物理内存，导致新的程序无法被装载，这时就会出现外部碎片的问题。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812152629864.png" alt="image-20240812152629864" style="zoom:50%;" /> 

内存分页将整个虚拟内存和物理内存空间分成一段段固定大小的片，虚拟内存和物理内存的映射以这个片为最小单位进行管理，我们把这个片称为页，在linux系统上，页的大小为4KB。 内存分页由于内存空间都是预先划分好的，页与页之间是紧密排列的，也就不会像内存分段一样，在段与段之间会产生间隙非常小的内存，所以不会有外部碎片。 但是，因为内存分页机制分配内存的最小单位是一页，即使程序不足一页大小，最少也只能分配一个页，所以页内会出现内存浪费，所以分页机制会产生内部碎片的问题。

 对于如何减少内存碎片？在操作系统层面，Linux系统使用伙伴系统Buddy分配器以页为单位来组织物理内存页框，对物理内存页进行合理的分配和回收，让内存分配与相邻内存合并能快速进行，用于缓解外部内存碎片，同时为了减少内部碎片，Linux还引入了slab算法，将内存页拆分为更小的单位来管理，slab可以对小对象空间进行分配，而不需要分配整个页面给对象，这样可以节省空间，内核中对于频繁使用的小对象，slab还会对此作缓存，避免频繁的内存分配和回收。

#### 硬链接和软链接

在Linux系统中，一切皆文件，不仅普通的文件和目录，块设备、管道、socket等等，都是由文件系统管理的。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812153045237.png" alt="image-20240812153045237" style="zoom:50%;" />

在 Linux 文件系统中，一个文件由目录项、索引节点和数据块三部分组成。

1 目录项（dentry），用来记录文件的名字，索引节点指针以及与其他目录项的层级关联关系。多个目录项关联起来，就会形成目录结构。目录项包括文件名和inode节点号。

2 索引节点（inode），用来记录文件的元信息，比如inode编号、文件大小、访问权限、创建时间、修改时间、数据在磁盘的位置等。

3 数据块，包含文件的具体内容，存储在硬盘上。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812153221687.png" alt="image-20240812153221687" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812153312574.png" alt="image-20240812153312574" style="zoom:50%;" />

在Linux中，元数据中的 inode号才是文件的唯一标识，而并非文件名。文件名仅是为了方便人们的记忆和使用，访问一个文件时，文件系统先在目录项中根据文件名找到对应的 inode 号，然后通过 inode 号获取到 inode 信息，最后根据 inode 信息找到文件数据所在的数据块。 链接是一种文件共享的方式，主流文件系统都支持链接文件。在linux系统中，链接分两种 ：一种被称为硬链接（Hard Link），另一种被称为符号链接或软链接（Symbolic Link）。 硬链接是某个文件实体的别名，硬链接可以使多个文件名拥有相同的inode，可以为单个文件创建多个硬链接。硬链接与普通文件没什么不同，inode 都指向同一个文件在硬盘中的区块。因为在不同的文件系统中，inode可能产生冲突，所以不能为不同文件系统或分区的目录和文件创建硬链接。 软链接和原文件不是一个文件，它是一种特殊的文件， 它有自己的inode和数据块，它的数据块是它所连接的文件的路径。当用户访问这个文件时，系统会自动将其替换成其所指的文件路径，如果原始文件被删除，所有指向它的符号链接也就都被破坏了。

#### 用户态和内核态

在 CPU 的所有指令中，有些指令是非常危险的，比如清除内存、设置时钟等，如果错用，将导致系统崩溃，如果允许所有程序都可以使用这些指令，那么系统崩溃的概率将大大增加。所以，CPU 将指令分为特权指令和非特权指令，对于那些危险的指令，只允许操作系统及其相关模块使用，普通应用程序只能使用那些不会造成灾难的指令。

比如 Intel 的 CPU 将特权等级分为 4 个级别：Ring0~Ring3。 对于Linux 系统，只使用了 Ring0 和 Ring3 两个运行级别。当进程运行在 Ring3 级别时被称为运行在用户状态，而运行在 Ring0 级别时被称为运行在内核态。 用户态运行的进程可以直接读取用户程序的数据，拥有较低的权限。当应用程序需要执行某些需要特殊权限的操作，例如读写磁盘、网络通信等，就需要向操作系统发起系统调用请求，进入内核态。 内核态运行的进程拥有非常高的权限，能够执行更底层、更敏感的操作，几乎可以访问计算机的任何资源，包括系统的内存空间、设备、驱动程序等。当操作系统接收到进程的系统调用请求时，就会从用户态切换到内核态，执行相应的系统调用，并将结果返回给进程，最后再从内核态切换回用户态。

 用户态切换到内核态有 3 种方式： 

1 系统调用。系统调用是用户态进程主动要求切换到内核态的一种方式，用户态进程通过系统调用向操作系统申请资源完成工作，比如读取磁盘资源。系统调用的机制核心还是使用了操作系统为用户特别开放的一个中断来实现。 2 中断。当 C P U 在执行用户态的进程时，外围设备完成用户请求的操作后，会向 C P U 发出相应的中断信号，这时 C P U 会暂停执行下一条即将要执行的指令，转到与中断信号对应的处理程序去执行，也就是切换到了内核态。如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后边的操作等。 

3 异常。当 CPU 在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关进程中，也就转到了内核态，比如缺页异常。在系统的处理上，中断和异常类似，都是通过中断向量表来找到相应的处理程序进行处理。区别在于，中断来自处理器外部，不是由任何一条专门的指令造成，而异常是执行当前指令的结果。

#### 动态链接和静态链接

静态链接和动态链接的区别是什么？

一个C/C++文件要经过预处理、编译、汇编和链接4步才能变成可执行文件。

程序代码首先经过预处理器生成 .i 文件。然后通过编译器生成 .s 汇编文件，再通过汇编器生成 .o 目标文件，最后通过链接器链接生成可执行文件。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812163029490.png" alt="image-20240812163029490" style="zoom:50%;" />

在链接阶段，根据链接过程中处理方式的不同，链接可以分为静态链接和动态链接。

静态链接是在链接阶段，就把所有需要的函数的二进制代码都包含到可执行文件中去，完成所有符号引用的一种链接方式。

而动态链接在链接的时候不直接拷贝相应依赖库的代码，而是通过记录一系列符号和参数，在程序运行或加载时将这些信息传递给操作系统，由操作系统负责将需要的依赖库加载到内存中，然后程序在运行到指定代码时，去执行内存中已经加载的依赖库的代码，最终达到运行时链接的目的。 

静态链接和动态链接对应的链接库也不同，可分为两种，分别是静态链接库和动态链接库，在Linux中，静态链接库文件以.a作为文件后缀，动态链接库以.so作为文件后缀。 静态库会在链接阶段将静态库中的代码复制到可执行文件中，当程序在执行的时候，在可执行文件中本身就有了静态库的代码，可以直接调用。 动态库则不会将代码打包到可执行文件中，而是打包动态库的名称等信息，在可执行程序运行的时候，需要去找到动态库的文件，然后把动态库加载到内存中才可以使用动态库中的代码。 静态库被打包到可执行程序中，生成的可执行文件较大，但是程序加载运行速度会比较快，发布程序时，也无需提供静态库，移植比较方便。但是当多个程序链接同一个静态库时，生成的每一个可执行文件中，都会含有这个静态库，相当于在内存中同时运行着两个相同的静态库，比较浪费系统资源。

另外，当静态库的内容发生更新时，依赖该静态库的程序也需要重新进行链接，导致程序的更新升级会比较麻烦。 动态库可以实现进程间资源共享，比如可执行程序A在运行时用到了某个动态库，那么系统会将该动态库的代码动态加载到内存中，此时若正在运行的程序B也需要用到该动态库中的代码，则不需要再次加载该动态库，而是可以与程序A共享。此外，当动态库的内容更新时，只需重新编译生成新的动态库即可，而不需要对依赖该动态库的程序重新进行编译链接。

#### 虚拟内存

虚拟内存，就是机器上运行的一个个的进程，访问的都是虚拟的内存，比如C语言中的指针指向的内存地址，或者gdb调试工具看到的地址，都是虚拟的，并不是机器上的实际物理内存。

虚拟内存是相对于物理内存来说的，物理内存，简单来说就是电脑上看到的那些内存条，是机器真正可以实际访问的物理内存空间。比如内存条是 8G 的，那计算机可用的物理内存就是 8G。

在一台计算机上，如果只有单个进程独享整个物理内存，当然是没问题的，但是现在的操作系统都是支持多进程并发运行的，当两个进程同时对同一块物理内存进行读写时，显然是有冲突的。此外，进程申请的内存可能还并没有使用，如果有太多的进程同时申请了大量的内存，也会导致物理内存不够用。

为了防止多进程运行时造成的内存地址的冲突，操作系统引入了虚拟内存，为每个进程提供了一个独立的虚拟内存空间，使得进程以为自己独占全部内存资源。

引入虚拟内存后，进程访问的虚拟内存地址通过CPU内部集成的内存管理单元MMU，转换成物理地址，然后再通过物理地址访问内存。

对于系统上运行的进程来说，在32位系统上，可以拥有4GB虚拟内存空间，在64位系统上，则可以拥有256T虚拟内存空间。

由于每个进程都有一个这么大的地址空间，导致所有进程的虚拟内存加起来，自然要比实际物理内存大得多。所以，并不是所有的虚拟内存都会分配物理内存，只有那些实际使用的虚拟内存才会分配物理内存。

当进程对某块虚拟内存进行读写时，CPU 就会去访问这块内存， 这时如果发现这块虚拟内存没有映射到物理内存， CPU 就会产生缺页中断，进程会从用户态切换到内核态，并将缺页中断交给内核的缺页中断函数处理，这时才会真正地为它分配物理内存。

操作系统使用分段和分页的机制管理虚拟地址与物理地址的映射关系。

内存分段机制，简单理解就是根据程序申请使用内存的需要，来把物理内存分成一段一段内存来管理，比如程序需要100M的内存，分段机制就给1段100M连续空间的物理内存与之对应。

内存分页将整个虚拟内存和物理内存空间分成一段段固定大小的片，虚拟内存和物理内存的映射以这个片为最小单位进行管理，我们把这个片称为页，在Linux系统上，页的大小为4KB。

此外，为了解决页表过大的问题，操作系统引入了多级页表机制。

为了解决页表访问慢的问题，在CPU中还加入了TLB页表缓存机制。

#### 虚拟内存布局

在32位系统上，进程拥有4GB虚拟内存空间，在64位系统上，则可以拥有256T虚拟内存空间。在进程整个虚拟内存空间中，又可以分为内核空间和用户空间两部分。32 位系统的内核空间占用 1G，位于最高处，剩下的 3G 是用户空间。64 位系统只使用了低 48 位，内核空间和用户空间都是 128T，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。

进程在用户态时，只能访问用户空间内存；只有进入内核态后，才可以访问内核空间内存。虽然每个进程的地址空间都包含了内核空间，但这些内核空间，其实关联的都是相同的物理内存。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。

对于进程虚拟内存的用户空间，从低往高，我们又可以分六个不同的内存段。

1 代码段，代码段用来存放程序执行代码，也可能包含一些只读的常量。这块区域的大小在程序运行时就已经确定，并且为了防止代码和常量遭到修改，代码段被设置为只读。

2  数据段，数据段用来存放程序中已初始化的全局变量与静态变量。

3  BSS 段， BSS段用来存放程序中未初始化的全局变量和静态变量。

4 堆，堆是动态内存分配区域，用来存放动态分配的内存，堆内存由用户申请和释放，从低地址向高地址增长。

5 文件映射段，文件映射段也叫共享区，主要包括共享内存、动态链接库等共享资源，从低地址向高地址增长。

6 栈，栈用来存放程序中临时创建的局部变量，如函数的参数、内部变量等。每当一个函数被调用时，就会将参数压入进程调用栈中，调用结束后返回值也会被放回栈中。同时，每调用一次函数就会创建一个新的栈，所以在递归较深时容易导致栈溢出。栈内存的申请和释放由编译器自动完成，并且栈容量由系统预先定义。栈从高地址向低地址增长。

堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 malloc() 或者 mmap() ，就可以分别在堆和文件映射段动态分配内存。 

#### 堆内存和栈内存区别

操作系统为了防止多进程运行时造成的内存地址冲突，引入了虚拟内存地址，为每个进程提供了一个独立的虚拟内存空间，使得进程以为自己独占全部内存资源。

对于进程虚拟内存的用户空间，从低往高，我们可以分六个不同的内存段，分别是代码段、数据段、BSS段、堆、文件映射段和栈。

在这六个内存段中，堆和栈是编程中经常用到的，也是最容易出错的地方。 栈用来存放程序中临时创建的局部变量，如函数的参数、内部变量等。每当一个函数被调用时，就会将参数压入进程调用栈中，调用结束后返回值也会被放回栈中。同时，每调用一次函数就会创建一个新的栈，所以在递归较深时容易导致栈溢出。栈内存的申请和释放由编译器自动完成，并且栈容量由系统预先定义，一般默认只有8M。栈从高地址向低地址增长。 栈内存为线程留出临时空间，每个线程都有一个固定大小的栈空间，而且栈空间存储的数据只能由当前线程访问，所以它是线程安全的。 堆内存是动态内存分配区域，用来存放动态分配的内存，堆内存由用户申请分配和释放，从低地址向高地址增长。在C和C++语言中使用malloc和new分配的内存默认就是堆内存。 堆内存的特点是大小不固定，可以动态扩容，空间由程序员动态分配，更加灵活。但是堆内存容易产生内存碎片，在分配和回收时需要对很多内存碎片进行整理，效率较低，所以我们一般都会通过内存分配器向堆区申请内存，比如通过glibc提供的malloc接口来动态申请内存。 此外，使用堆内存，还特别需要注意两个问题： 1、堆内存容易产生内存泄露，malloc出来的内存如果没有free，new出来的内存如果没有delete，都会产生内存泄露。 2、堆内存不像栈内存是线程独立的，它是线程不安全的，堆内存可以被一个进程内所有的线程访问，多线程操作时可能会产生问题。

#### malloc/free 和new/delete区别

C和C++编程中，malloc/free和new/delete虽然都可以用来申请和释放内存，但是它们的原理以及使用上都存在着很多的不同。 

具体来看，它们主要有5个不同点：

 1、属性不同 malloc/free是glibc库提供的库函数，使用时需要引入相应的头文件。 new/delete不是函数，而是C++中的关键字，使用时不需要依赖头文件，但是需要编译器支持。

 2、使用上的区别 malloc申请内存空间时需要显式填入申请内存的大小。malloc内存分配成功时返回void * ，需要通过强制类型转换，将void*指针转换成我们需要的类型。malloc分配内存失败时返回NULL，我们可以通过返回值判断内存是否分配成功。 new会根据new的类型来分配内存，所以无需显式填入申请的内存大小。new操作符内存分配成功时，返回的是对象类型的指针，类型严格与对象匹配，无须进行类型转换，所以new是符合类型安全性的操作符，在C++程序中使用new会比malloc安全可靠。new内存分配失败时，不会返回NULL，而是抛出异常，如果不捕捉异常，那么程序就会异常退出。 

3、内存位置的区别 malloc申请的内存是在堆空间。堆是操作系统分配给进程的一块特殊内存区域，它提供了动态分配的功能，当运行程序调用malloc()时就会从中分配，调用free()归还内存。 new分配的内存空间是在自由存储区。自由存储区是C++中动态分配和释放对象的一个概念，通过new分配的内存区域可以称为自由存储区，通过delete释放归还内存。自由存储区可以是堆，也可以是全局静态存储区，具体是在哪个区，要看new的实现以及C++编译器默认new申请的内存是在哪里。很多C++编译器默认使用堆来实现自由存储，new和delete内部默认使用malloc和free的方式来被实现。

 4、是否可以重载 在C++中new和delete符号是可以重载的，所以可以重新实现new的实现代码，让其分配的内存位置在静态存储区等。而malloc和free是C里的库函数，无法对其进行重载。

 5、是否可以动态扩充内存

使用malloc分配内存后，如果发现内存不够用，可以通过realloc函数来扩充内存大小。而new没有扩充内存的机制。

#### 原子操作如何实现

现代计算机系统中，通过使用高速缓存cache进行数据交互，解决了CPU和内存之间速度的矛盾。在这种多核系统中，每个处理器都有自己的Cache，但它们共享同一主存，当多个 CPU 操作同一份数据时，各个 CPU 缓存中同一份数据的值可能会存在不一致的情况，也就是存在缓存一致性的问题。

比如多个线程对某个共享变量进行加操作，因为这种操作不是原子的，操作完之后，共享变量的值就会与期望值不一致。

原子操作表示操作是一个整体，不可再分。在并发场景下，如果无法保证某些操作是原子的，数据就无法一致，导致系统不可用。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812170006571.png" alt="image-20240812170006571" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812170052941.png" alt="image-20240812170052941" style="zoom:50%;" />

CPU实现原子操作的方式一般有两种：

1、总线锁定

总线锁定是指处理器提供一个LOCK#信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞，那么这个处理器就可以独占这个共享内存。这个锁的是CPU与内存之间的通信，就等于将其他处理器与内存的所有交互都锁住了。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812170118166.png" alt="image-20240812170118166" style="zoom:50%;" />

在同一时刻，其实我们只需保证对某个内存地址的操作是原子性即可，但总线锁定把CPU和内存之间的通信锁住了，这使得锁定期间，其他处理器不能操作其他内存地址的数据，所以总线锁定的开销会比较大。

2、缓存锁定

频繁使用的内存会缓存在处理器的高速缓存里，那么原子操作就可以直接在处理器内部的缓存中进行，而并不需要声明总线锁。如果内存区域被缓存在处理器的缓存行中，并且在Lock操作期间被锁定，那么当它执行锁操作回写到内存时，处理器不在总线上输出LOCK＃信号，而是修改内部的内存地址，并允许它的缓存一致性机制来保证操作的原子性，因为缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据，当其他处理器回写已被锁定的缓存行数据时，会使缓存行无效。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812170200554.png" alt="image-20240812170200554" style="zoom:50%;" />

虽然缓存锁定开销更小，但是有两种情况，处理器不会使用缓存锁定。 一种是当操作的数据不能被缓存在处理器内部，或操作的数据跨多个缓存行时，则处理器会调用总线锁定。 另一种是有些处理器不支持缓存锁定，就算锁定的内存区域在缓存行中，也会调用总线锁定。 Intel处理器提供了很多 LOCK 前缀的指令来实现总线锁定和缓存锁定。比如位测试和修改指令 BTS，BTR，BTC，交换指令 XADD，CMPXCHG等，被这些指令操作的内存区域就会加锁，导致其他处理器不能同时访问它。比如Java中实现原子操作的CAS（比较并交换）就是基于处理器提供的 CMPXCHG 指令来实现的。

# 计算机网络

#### 常见

Q1： TCP如何唯一确定一条连接？

TCP通过四元组唯一确定一条连接，四元组即源IP地址、目的IP地址、源端口和目的端口。



<img src="https://i1.hdslb.com/bfs/article/ac3e97515ea8571ef8591795c826d2ba1886950756.png@1192w.webp" alt="img" style="zoom:67%;" />

Q2:  TCP三层握手过程中，可以携带数据吗？

第一次、第二次握手不可以携带数据，第三次握手时，是可以携带数据的。

<img src="https://i1.hdslb.com/bfs/article/3e82a7d1156623269f840953f92c34a91886950756.png@1192w.webp" alt="img" style="zoom:67%;" />

Q3:  TCP 和 UDP 可以同时绑定同一个端口吗？

可以，TCP和UDP是两种不同的传输层协议，它们各自使用的端口号也相互独立，如TCP有一个80端口，UDP也可以有一个80端口，二者并不冲突。

<img src="https://i1.hdslb.com/bfs/article/d64ebc91293e20a00c008672df3a24541886950756.png@1192w.webp" alt="img" style="zoom:50%;" />



Q4:  多个TCP服务器进程可以同时绑定同一个端口吗？

要看它们绑定的IP地址是否相同，如果IP地址也相同，就不可以再绑定同一个端口。如果IP地址不相同，就可以。

<img src="https://i1.hdslb.com/bfs/article/fe77929fc9423ed111ed13053b3221121886950756.png@1192w.webp" alt="img" style="zoom:67%;" />



Q5:  客户端的端口可以重复使用吗？

可以重复使用。TCP通过四元组（源IP地址、目的IP地址、源端口和目的端口）唯一确定一条连接，只要四元组中的一个维度不一样，就表示不同的TCP连接。 所以，即使客户端的端口（即源端口）相同，只要其它三个维度有一个维度不相同，就不会导致连接冲突的问题。

<img src="https://i1.hdslb.com/bfs/article/b007ffb2a585d77c21d4d8cf5520e4eb1886950756.png@1192w.webp" alt="img" style="zoom:67%;" />



Q6: IP层已经有了分片机制，TCP为什么还需要分段？

因为IP层本身没有超时重传机制，超时和重传是由TCP负责的，当某一个IP分片丢失后，TCP就得重传整个IP报文的所有分片，效率太低。所以一般都是UDP使用IP分片，TCP使用分段而不使用分片。TCP在建立连接时，会根据网卡MTU的大小协商双方的MSS值，超过MSS时，就会先进行分段，由它形成的IP包的长度也就不会大于MTU ，也就不用进行IP分片了。

<img src="https://i1.hdslb.com/bfs/article/f9da2ba62fce96ef24ed7851bb407a201886950756.png@1192w.webp" alt="img" style="zoom:67%;" />



<img src="https://i1.hdslb.com/bfs/article/1c33c02d97023dd8d378639fe3ef3c901886950756.png@1192w.webp" alt="img" style="zoom:67%;" />

Q7:  为什么有TIME_WAIT状态？为什么等待的时间是2MSL？

TIME_WAIT是TCP四次挥手过程中，主动发起结束连接的一方，在发送第四次挥手ACK包之后的状态。处于TIME_WAIT状态的一方，在经过2个MSL（报文最大生存时间）时间后，才变为CLOSED状态，结束连接。

<img src="https://i1.hdslb.com/bfs/article/40dcb0d67606686236cd823d4495f6a91886950756.png@1192w.webp" alt="img" style="zoom:67%;" />

在TIME_WAIT状态时，两端的端口不能使用，要等到2MSL时间结束才可继续使用，而2MSL的时间足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，所以可以保证历史连接的报文不会影响新的连接。

<img src="https://i1.hdslb.com/bfs/article/a58fc893f7b8856ab1d0c91536bae1571886950756.png@1192w.webp" alt="img" style="zoom:67%;" />

等待2MSL时间主要目的是怕最后一个ACK包对方没收到，那么对方在超时后将重发第三次握手的FIN包，主动关闭的一方接到重发的FIN包后可以再发一个ACK应答包。

#### Tcp和Udp区别是什么

TCP（传输控制协议）和UDP（用户数据报协议）是在网络传输中常用的两个基于IP协议的传输层协议。 

TCP是一种面向连接的协议，通过建立可靠的连接来传输数据。而UDP是一种无连接的协议，数据包发送之前不需要建立连接。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812202826928.png" alt="image-20240812202826928" style="zoom: 33%;" />

 TCP提供可靠的数据传输，它使用**序号、确认和重传机制**来确保数据的完整性和可靠性。UDP不提供可靠性保证，数据包发送后不能得到确认或重传。 UDP相对于TCP更加轻量级，没有TCP的连接建立和确认过程，因此传输数据速度更快。

 TCP需要维护连接状态和传输控制信息，因此消耗的系统资源较多。UDP则简单高效，消耗的系统资源较少。 

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812204022918.png" alt="image-20240812204022918" style="zoom:33%;" /><img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812204151593.png" alt="image-20240812204151593" style="zoom:33%;" />

应用场景上， TCP适用于对可靠性要求较高的应用，如文件传输、电子邮件、网页浏览等。TCP在数据传输过程中能够确保数据的顺序和完整性，适合处理大量的数据和事务类工作。UDP适用于对实时性要求较高、但对可靠性要求不高的应用，如音频/视频流媒体、在线游戏等。UDP的高速传输和低延迟特性对实时性要求高的应用非常有利。

#### HTTP和HTTPS区别

HTTP:  HTTP是一种用于在Web浏览器和Web服务器之间交换数据的应用层协议。

通过HTTP，Web浏览器可以向Web服务器发送请求并获取响应，从而实现Web页面的访问和传输。HTTP使用TCP作为传输层协议，并采用请求-响应模型来进行通信。

 HTTPS：HTTP使用明文传输数据，存在一些潜在的安全威胁和漏洞，比如窃听、篡改、重放攻击等。

为了弥补这些漏洞，人们提出了HTTPS协议来保证HTTP通信的安全性。HTTPS采用SSL/TLS协议对HTTP通信进行加密，并通过数字证书验证来保证通信双方的身份和数据的完整性和保密性。 区别上，HTTP协议是以明文的方式传送数据，不对数据提供任何形式的数据加密，容易被攻击者截取浏览器和服务器之间的传输报文，造成用户敏感信息泄露,例如信用卡号、密码等。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812213111087.png" alt="image-20240812213111087" style="zoom: 33%;" />

HTTPS在HTTP的基础上加入了SSL协议，SSL依靠证书来验证浏览器和服务器的身份，并对传送的数据加密，从而大大提高数据传输方面的安全性。 但是，由于HTTPS在中间加上了SSL/TLS安全层，使得其建立连接的过程更复杂，相对于HTTP，HTTPS的页面响应速度会慢一些，同时也会消耗更多的服务器资源。 此外，HTTPS 和 HTTP 使用的是完全不同的连接方式，用的端口也不一样，HTTPS一般使用 443端口，HTTP使用 80或者8080端口。

#### HTTP方法GET,POST

在 HTTP 协议中，常见的请求方法有 GET、POST、PUT 、 PATCH 、DELETE 等。它们各自具备不同的语义，分别用于不同的场景和目的。其中，GET方法用于请求资源，POST 方法用于创建资源，PUT 方法用于更新资源，而 PATCH 方法则用于部分更新资源。 

GET 方法用于请求指定的资源，并返回响应主体，GET是安全且幂等的。安全是指GET操作用于获取信息而非修改信息，就像数据库查询一样，不会修改和增加数据，不会影响资源的状态。幂等是指对同一个 URL 的多个请求应该返回同样的结果。因为GET 请求安全而幂等，所以它可被浏览器缓存，可保留在浏览器历史记录中，也可被收藏为书签。

 POST 方法用于向指定资源提交数据，请求服务器进行处理（例如提交表单或者上传文件），数据被包含在请求本文中。POST 因为是新增或提交数据的操作，会修改服务器上的资源，所以是不安全的，且多次提交数据就会创建多个资源，比如将一个订单重复提交多次，所以它不是幂等的。因为POST请求是不安全不幂等的，所以它不可被浏览器缓存，不可保留在浏览器历史记录中，也不可被收藏为书签。 PUT 方法用于将数据发送到服务器来更新资源，PUT 方法在更新资源时会完全替换原有的资源，需要注意不能遗漏任何属性或字段。PUT 与 POST 方法的区别在于，PUT 方法是幂等的，调用一次与连续调用多次是等价的，即没有副作用。

PATCH 方法用于对资源进行部分修改。与 PUT 方法相比，PATCH 方法更加轻量级，它只需要传输要更新的属性或字段即可。PATCH 方法的请求体中只包含要更新的属性或字段，不需要传输完整的资源表示。不同于 PUT 方法，而与 POST 方法类似，PATCH 方法是非幂等的，这就意味着连续多个的相同请求会产生不同的效果。 整体来看，GET、POST、PUT 和 PATCH 方法分别用于请求、创建、更新资源和部分更新资源，它们在请求时需要传输不同类型的数据，以满足各自的语义性。另外，在使用上我们得知道，GET请求是安全的，而POST、PUT和PATCH都是非安全的，GET和PUT是是幂等的，而POST和PATCH是非幂等的。

#### 三次握手

TCP 三次握手是指 TCP 在传递数据之前，需要进行 3 次交互才能正式建立起连接，并进行数据传递。 

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812215149181.png" alt="image-20240812215149181" style="zoom:50%;" />

三次握手具体过程是： 

1 客户端向服务器发送一个 SYN 包， 表示客户端请求建立连接。 

2 服务器收到客户端的 SYN 包后，向客户端发送一个 SYN+ACK 包，表示服务器接收到了客户端的请求，并同意建立连接。 

3 客户端收到服务器的 SYN+ACK 包后，向服务器发送一个 ACK 包，表示客户端确认收到了服务器的响应 TCP

 之所以需要 3 次握手是因为 TCP 双方都是全双工的。所谓全双工指的是，TCP 任何一端既是发送数据方，又是接收数据方，因此这就要求 TCP 通讯双方既要保证自己的发送能力，又要保证自己的接收能力才行。 这就好像打电话时，通讯双方都要保证自己传递声音的话筒和接收声音的耳机都是正常的才行，这样才能进行有效的交流。三次握手的主要目的是确认自己和对方的发送和接收都是正常的，从而保证了双方能够进行可靠通信。 若采用两次握手，当第二次握手后就建立连接的话，此时客户端知道服务器能够正常接收到自己发送的数据，而服务器并不知道客户端是否能够收到自己发送的数据。 此外，网络往往是非理想状态的，可能存在丢包和延迟，客户端发送建立连接请求，由于网络拥塞，迟迟没有得到回应，客户端再次发送连接请求，服务端回应，连接建立。 一段时间后，客户端第一次发送的连接请求到达服务端，服务端以为客户端重新请求建立连接，但其实并没有，此时服务端会返回响应报文并一直处于待连接状态，这就造成了服务器资源的浪费。 可见，TCP采用三次握手而不是两次握手，主要原因是为了防止已失效的连接请求报文突然又传送到了服务端，从而导致错误。当然，三次握手就可以解决通信连接的问题，为了提高通信效率，也就没有必要设计成四次握手、五次握手了。

#### 四次挥手

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812222804895.png" alt="image-20240812222804895" style="zoom:50%;" />

TCP 四次挥手是指 TCP 在数据传输完成之后，需要进行 4 次交互才能最终断开连接。 客户端或服务端均可主动发起挥手动作，以客户端主动发起挥手为例，四次挥手具体过程是： 1 客户端发送一个 FIN 包，表示希望关闭连接。 2 服务器收到客户端的 FIN 包后，向客户端发送一个 ACK 包，表示服务器已经收到了客户端的请求。 3 服务器在发送完 ACK 包之后，也会发送一个 FIN 包，表示服务器也希望关闭连接。 4 客户端收到服务器的 FIN 包后，向服务器发送一个 ACK 包，表示客户端已经收到了服务器的请求。 终止一个 TCP 连接要经过四次挥手是由于 TCP 的半关闭（half-close）特性造成的，TCP 提供了连接的一端在结束它的发送后，还能接收来自另一端数据的能力。 任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送的时候，则发出连接释放通知，对方确认后就完全关闭了TCP连接。 通俗的来说，两次握手就可以释放一端到另一端的 TCP 连接，完全释放连接一共需要四次握手。 四次挥手和我们打电话时结束通话的过程很像，例如，小明和小红打电话聊天，通话差不多要结束时，小红说，“我没啥要说的了”。小明回答，“我知道了”。但是小明可能还有要说的话，这时小红不能马上结束通话，于是小明可能又叽叽歪歪说了一通，最后小明说，“我说完了”，小红回答，“我知道了”，这样通话才算结束。

#### HTTP协议和RPC协议

HTTP协议（Hyper Text Transfer Protocol），又叫做超文本传输协议。是一种用于在Web浏览器和Web服务器之间交换数据的应用层协议。通过HTTP，Web浏览器可以向Web服务器发送请求并获取响应，从而实现Web页面的访问和传输。HTTP使用TCP作为传输层协议，并采用请求-响应模型来进行通信。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812224627070.png" alt="image-20240812224627070" style="zoom: 33%;" />

RPC（Remote Procedure Call），又叫做远程过程调用，它允许客户端在不知道调用细节的情况下，调用存在于远程计算机上的某个对象，就像调用本地应用程序中的对象一样。RPC的调用协议通常包含传输协议和序列化协议。

RPC并不是一个具体的协议，而是一种调用方式，它并没有具体实现，只要按照 RPC 通信协议规范实现的框架，都属于RPC，比如 Dubbo、gRPC 等。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812224729684.png" alt="image-20240812224729684" style="zoom:33%;" />

整体上看，HTTP和RPC的主要区别5点：

1、基于的通信协议不同，HTTP只能基于HTTP协议，而RPC可以基于HTTP、TCP和UDP协议。 

2、调用方式不同，HTTP 接口通过 URL 进行调用，RPC 接口通过函数调用进行调用。 

3、使用场景上不同，HTTP主要用于 B/S 架构，是万维网数据通信的基础，服务在网页端和服务端的数据传输上 。而 RPC 更多用于 C/S 架构，多用于分布式系统内部集群里，例如云计算、微服务架构、分布式数据库等，它可以在不同的服务之间进行远程调用，从而实现分布式系统的协作。 

4、传输效率上，RPC使用自定义的TCP协议，请求报文体积更小，可以很好地减少报文体积，提高传输效率。而HTTP请求中会包含很多无用的内容。 

5、性能上，RPC协议通常使用二进制编码来传输数据，相对于HTTP协议的文本传输，RPC具有更高的性能和效率。RPC协议通常采用高效的序列化和反序列化技术，减少了数据传输的大小和开销，提高了通信的速度和响应时间。

#### 请说一下，HTTP协议中Cookie和Session的区别是什么？

HTTP协议是一种无状态协议，即每次服务端接收到客户端的请求时，都是一个全新的请求，服务器并不知道客户端的历史请求记录。

Cookie和Session都是用来跟踪浏览器用户身份的会话方式，目的就是为了弥补HTTP的无状态特性。 Cookie是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器下次向同一服务器再发起请求时被携带。 Cookie中保存已经登录过的用户信息，下次访问网站的时候，页面可以自动填写登录的一些基本信息。通常，它用于告知服务端两个请求是否来自于同一浏览器，如保持用户的登录状态。此外，Cookie还能保存用户首选项，主题和其他设置信息。 Session的作用是通过服务端记录用户的状态。一般我们会使用Cookie来管理Session，服务器第一次接收到请求时，生成一个Session ID ，通过响应头的Set-Cookie命令设置Session ID字段，并向客户端发送要求设置Cookie的响应。客户端收到响应后，在本机保存一个包含Session ID字段的Cookie信息，接下来客户端每次向同一服务器发送请求时，请求头都会带上包含该Session ID的Cookie，然后服务器通过读取请求头中的Cookie，获取到此次请求的Session ID。 需要注意的是，如果客户端禁用了Cookie，通过Cookie保存Session ID的方式就无法使用了，这时我们也可以把Session ID放在请求的URL里面，考虑到安全性，我们还可以对Session ID进行加密。 

整体上看，Cookie和Session存在5点区别： 1 存放位置不同，Cookie 数据保存在客户端浏览器上，而Session 数据保存在服务器上。 2 安全性不同，Cookie存放在本地浏览器上，可以对其进行伪造从而进行Cookie欺骗，所以相对来说，Session安全性更高。 3  存储数据大小不同，单个Cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个Cookie，而Session则存储于服务端，浏览器对其没有限制。 4 数据类型不同， Cookie 只支持存储字符串数据，而 Session 可以存储任意数据类型。 5 有效期不同，Cookie 可设置为长时间保持，比如我们经常使用的自动登录功能，Session 一般生效时间较短，客户端关闭或者 Session 超时都会失效。

#### 请说一下，浏览器从输入URL到展示页面，经历了哪些过程？

浏览器根据输入的URL通过HTTP或者HTTPS协议发起对远端Web服务器的请求，Web服务器返回对应请求的数据给浏览器，然后浏览器将数据解释渲染并最终展示给用户。整个过程涉及到网络中各种协议的交互以及各种设备对报文的转发，主要流程有：

 **1、浏览器进行URL解析和生成HTTP请求** 浏览器首先对URL进行解析，确定要访问的Web服务器和文件名，然后生成要发送给Web服务器的请求信息。当然，在生成HTTP请求之前，浏览器还会查看本地缓存是否已经缓存有要请求的资源，如果有，就直接返回缓存资源显示出来。如果没有，才会继续进行后面的请求流程。

 **2、生成TCP报文段,准备发起TCP连接** HTTP是基于TCP 协议传输的，在做完应用层HTTP请求的准备后，客户端主机开始进行TCP报文段头部的封装。 

**3、DNS解析域名得到服务器的IP地址** 传输层TCP报文段组装好后，开始进入网络层进行IP数据报的组装，但是URL中请求服务器资源使用的是域名，而网络中标识服务器位置使用的是IP地址，所以先要通过DNS域名解析协议进行域名解析，先获取到服务器域名对应的IP地址。 DNS进行域名解析的过程是，先查浏览器DNS缓存，再查hosts主机文件，最后查DNS服务器，由于DNS服务器采用的是层级结构，所以DNS查询采用的是递归迭代查询。 

**4、IP路由寻址和生成IP数据报** 传输层TCP处理完后，开始进行网络层IP数据报的处理。特别是对于多网卡客户端主机，访问服务器的IP数据报需要确定从哪个网卡发出去，所以需要进行路由寻址查找路由表确定报文发出去的网口，从而确定IP数据报的源IP地址。

 **5、ARP请求下一跳网关MAC地址** IP数据报组装完成之后，需要进行以太网帧的封装，以太网帧头的源MAC就是报文发出网卡的MAC，而目的MAC就是路由表中指定的下一跳网关IP对应的MAC，这时候需要通过ARP地址解析协议来请求获取下一跳网关的MAC的地址，从而确定以太网帧头的目的MAC。 

**6、报文经过网络发往WEB服务器** 经过前面的各种处理，请求报文已经组装完成，现在可以从客户端主机网卡发送出去了，整个网络是经过大量的交换机和路由器互联的，报文会经过这些设备的转发最终到达Web服务器。 

**7、服务器返回响应报文** 服务器收到请求报文后，首先检查报文的目的MAC是否是自己网卡的MAC，目的IP是否是自己网卡的IP，然后查看传输层TCP协议的端口，确认是Web服务进程监听的端口，所以将请求报文交给Web服务进程进行处理。Web服务进程将请求的网页内容封装成响应报文，然后通过网卡发出去，响应报文经过网络中的交换机和路由器转发到达客户端主机，并最终通过客户端主机的浏览器将响应报文中的网页内容渲染展示出来。 整体上看，浏览器输入URL到展示网页内容，需要经过浏览器解析URL、DNS解析域名IP地址、查找路由表确定出口网卡和源IP地址、ARP地址解析获取主机网关下一跳的MAC地址、TCP连接建立以及交换机路由器等网络中间设备对报文的转发这些过程。

#### HTTP协议版本

HTTP协议，又叫做超文本传输协议。是一种用于在Web浏览器和Web服务器之间交换数据的应用层协议。

HTTP协议到目前为止，所有的版本可以分为HTTP 0.9、1.0、1.1、2.0、和3.0，其中普遍应用的是HTTP 1.1版本，正在推进是HTTP 2.0版本，以及未来的HTTP 3.0版本。

HTTP1.0规定浏览器和服务器保持短连接，浏览器每次请求都需要与服务器建立一个TCP连接。HTTP1.0还规定下一个请求必须在前一个请求响应到达之前才能发送，如果前一个请求的响应一直不到达，那么下一个请求就不发送，后面的请求就都阻塞了，所以HTTP1.0存在请求的队头阻塞。HTTP1.0还不支持断点续传，每次都会传送全部的页面和数据， 在只需要部分数据的情况下就会浪费多余带宽。

HTTP 1.1解决了1.0版本存在的问题，它可以保持长连接，避免每次请求都要重复建立TCP连接，提高了网络的利用率。HTTP 1.1 可以使用管道传输，支持多个请求同时发送，但服务器还是按照顺序先回应前面的请求，再回应后面的请求，如果前面的回应特别慢，后面就会有许多请求排队等着处理。所以，HTTP 1.1 还是存在响应的队头阻塞问题。另外，HTTP 1.1已经可以断点续传。

HTTP  2.0是HTTP协议的第一个主要修订版，它与前面的版本用于传递数据的方法有很大的差异。 HTTP2.0会压缩头部，如果同时有多个请求其头部一样或相似，那么协议会消除重复部分。 HTTP 2.0 将请求和响应消息编码为二进制，而不再使用之前的纯文本消息，增加了数据传输的效率。 HTTP 2.0可以在一个TCP连接中并发多个请求或回应，而不用按照顺序一一对应，从而彻底解决了HTTP层面队头阻塞的问题，大幅度提高了连接的利用率。 HTTP 2.0还在一定程度上改善了传统的请求应答工作模式，服务端不再是被动地响应，而是可以主动向客户端发送消息、推送额外的资源。

HTTP 2.0虽然通过多个请求复用一个TCP连接解决了HTTP的队头阻塞 ，但是一旦发生丢包，就会阻塞住所有的HTTP请求，这就属于TCP层队头阻塞。为了解决这个问题，HTTP 3.0直接放弃使用TCP，将传输层协议改成UDP，但是因为UDP是不可靠传输，所以这就需要QUIC实现可靠机制。

QUIC全称 “快速 UDP 互联网连接”，是由Google提出的使用UDP进行多路并发传输的协议。QUIC 有自己的一套机制可以保证传输的可靠性的。当某一对请求响应发生丢包时，只会阻塞当前的请求响应，其他请求响应不会受到影响，因此完全不存在队头阻塞问题。 HTTP 3 .0使用了UDP作为传输层协议，能够减少三次握手的时间延迟，从而达到快速建立连接的效果。此外，QUIC协议可以使用连接ID来标记通信的两个端点，即使移动设备的网络发送变化，导致IP地址变化了，只要还有连接ID和TLS密钥等上下文信息，就可以复用原连接，从而实现连接迁移。

#### 请说一下，DNS域名解析的过程是怎样的？

DNS协议是一种基于UDP的应用层协议，它用于将网站的网址也就是域名，转换为IP地址，以便用户可以访问网站。

DNS把域名和IP地址联系在一起，有了保存网站域名和IP地址对应关系的DNS服务器，我们就不用输入IP地址来访问一个网站，而是输入网址，然后通过向DNS服务器请求获取域名对应的IP来访问网站了。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812225523458.png" alt="image-20240812225523458" style="zoom:33%;" />

DNS域的命名空间是一种树状层次结构，一般可分为根域、一级域（也叫顶级域）、二级域、子域以及主机名。

对应地，在域名的每一层都会有一个域名服务器，来提供对应层级的域名解析服务。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812225603519.png" alt="image-20240812225603519" style="zoom:33%;" />

DNS进行域名解析的过程是，主机先查本地浏览器上的DNS缓存，再查本机操作系统中的DNS缓存以及hosts文件，如果都没有，才会向本地DNS服务器发起DNS域名解析请求，本地DNS服务器如果有请求域名对应的IP地址，则直接返回告诉主机。如果没有，就向根域名服务器发送请求，根域名服务器是最高层次的，它不直接用于域名解析，但是它会告诉本地DNS服务器去找对应的顶级域名服务器，接着，本地DNS服务器去找对应的顶级域名服务器请求，顶级域名服务器会告诉本地DNS服务器去找对应的权威DNS服务器，本地DNS于是再转向问权威 DNS服务器，权威DNS服务器查询后将对应的 IP 地址告诉本地 DNS，本地DNS再将IP地址返回给主机。

<img src="C:\Users\wangbanjin\AppData\Roaming\Typora\typora-user-images\image-20240812225704438.png" alt="image-20240812225704438" style="zoom:33%;" />

这个时候，为了避免下次对这个域名发起DNS解析时，再来一遍上面的解析过程，主机和本地DNS都会对刚刚的域名解析结果进行缓存，下次再解析时，如果有缓存就可以直接从缓存中获取解析结果。

从上面的过程可以看到，DNS的查询方式分为两种，一种是递归，一种是迭代。

递归查询指的是如果 A 请求 B，那么 B 作为请求的接收者，一定要给 A 想要的答案。

迭代查询指的是，如果接收者 B 没有请求者 A 所需要的准确内容，接收者 B 将告诉请求者 A，如何去获得这个内容，但是自己并不去发出请求。

在实际应用中，递归查询通常用于从请求主机到本地 DNS 服务器的查询，而迭代查询则用于本地 DNS 服务器向根域名服务器或者顶级域名服务器发出查询请求。

#### TCP是如何保证可靠传输的？

网络中存在多种因素可能导致数据在传输过程中丢失、损坏或乱序，如传输媒介的不稳定性、拥塞、丢包等。为应对这些问题，TCP引入了一系列机制来保证数据的可靠传输。网络性能四大指标为带宽，时延，抖动和丢包。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812225955413.png" alt="image-20240812225955413" style="zoom:33%;" />

1 连接管理机制

TCP是一种面向连接的可靠传输协议，TCP使用三次握手和四次挥手来建立和终止连接。通过三次握手，发送方和接收方交换序列号、窗口大小等信息，确保双方都准备好进行数据传输。在传输过程中，通过四次挥手正常终止连接，确保最后的数据能够完整传输。

2  序列号和确认应答机制

TCP通过给每个字节分配一个序列号来跟踪数据的传输。发送方按序列号将数据分割成多个报文段，并发送到网络中。接收方通过确认应答（ACK）机制告知发送方已成功接收到数据。如果发送方在一定时间内未收到确认应答，则会重新发送相应的数据。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812230038291.png" alt="image-20240812230038291" style="zoom:33%;" />

3  重传机制

为了确保数据的可靠传输，TCP在发送数据后启动一个定时器。如果在定时器时间内未接收到确认应答，则认为数据丢失，发送方会重新发送该数据（超时重传）。此外，如果收到同一个数据包的多次确认，说明也有数据丢失，也会触发重传（快速重传）。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812230113687.png" alt="image-20240812230113687" style="zoom:33%;" />

4 流量控制

流量控制，就是接收方调控发送方的发送速度不要太快的机制 。滑动窗口机制允许发送方在未收到确认应答之前发送多个数据报文段，提高传输效率。接收方通过窗口大小来告知发送方可以接收的数据量。发送方根据窗口大小进行流量控制，确保不会发送超出接收方处理能力的数据。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812230143006.png" alt="image-20240812230143006" style="zoom: 33%;" />

5 拥塞控制

TCP还通过拥塞控制机制来优化网络性能并避免网络拥塞。拥塞控制算法根据网络的拥塞情况自适应地调整发送方的发送速率，防止过多的数据注入网络，避免网络拥塞和数据丢失。

<img src="https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812230300771.png" alt="image-20240812230300771" style="zoom:33%;" />

TCP通过上述机制来保证数据的可靠传输，确保数据在网络中的完整性、一致性和正确性。无论是处理网络丢包、乱序、拥塞还是其他异常情况，TCP都能自适应地调整传输策略，提供可靠的传输服务。

#### 请说一下，TCP的连接状态是如何变化的？

TCP是一种面向连接的可靠传输协议，TCP使用三次握手和四次挥手来建立和终止连接。通过三次握手，发送方和接收方交换序列号、窗口大小等信息，确保双方都准备好进行数据传输。在传输过程中，通过四次挥手正常终止连接，确保最后的数据能够完整传输。

TCP客户端和服务器在建立连接和断开连接的过程中，会存在不同的状态迁移变化。

建立连接时的状态变迁：

1、建立连接之前服务器和客户端的状态都为CLOSED。

2、服务器创建socket后开始监听，变为LISTEN状态。

3、客户端请求建立连接，向服务器发送SYN报文，客户端的状态变为SYN_SENT。

4、服务器收到客户端的报文后，向客户端发送ACK和SYN报文，此时服务器的状态变为SYN_RCVD。

5、客户端收到服务器的ACK和SYN报文，就向服务器发送ACK，客户端状态变为ESTABLISHED

6、服务器收到客户端的ACK后也变为ESTABLISHED。

至此，3次握手完成，连接建立！

断开连接时的状态变迁（服务器也可以主动断开连接，以客户端主动断开连接为例）：

1、客户端先向服务器发送FIN报文，请求断开连接，其状态变为FIN_WAIT1。

2、服务器收到FIN后向客户端发送ACK，服务器状态变为CLOSE_WAIT。

3、客户端收到ACK后就进入FIN_WAIT2状态。此时连接已经断开了一半了。

4、如果服务器还有数据要发送给客户端，就会继续发送。直到发完了，就发送FIN报文，此时服务器进入LAST_ACK状态。

5、客户端收到服务器的FIN后，马上发送ACK给服务器，此时客户端进入TIME_WAIT状态，再过了2MSL长的时间后进入CLOSED状态（MSL指的是报文最大生存时间）。

6、服务器收到客户端的ACK就进入CLOSED状态。

至此，四次挥手完成，连接结束！

断开连接过程中，有两点需要注意：

1、如果客户端发送FIN报文后，在收到服务器的ACK之前先收到了服务器的FIN，此时客户端回复ACK给服务器，状态变为CLOSING状态，等客户端再收到服务器的ACK后，状态变为TIME_WAIT状态。

2、如果客户端发送FIN报文后，收到了服务器同时带有ACK和FIN标志的报文，可以直接进入到TIME_WAIT状态，而无须经过FIN_WAIT_2状态。

#### 请说一下，TCP序列号和确认号是如何变化的？

TCP是一种面向连接的可靠传输协议，序列号和确认号是保证TCP可靠传输的一种重要机制。在TCP协议中，每个数据包都有一个序列号seq和一个确认号ack。

<img src="https://i1.hdslb.com/bfs/article/watermark/35cb67d8e5831cb6675277d16c1cf1191886950756.png@1192w.webp" alt="img" style="zoom: 50%;" />

序列号表示这个数据包中的第一个字节在整个数据流中的位置。对于发送方来说，序列号用来跟踪已发送的字节数。而接收方则通过序列号来确定自己是否接收到正确的数据。

<img src="https://i1.hdslb.com/bfs/article/fb26df0a01f8cef752ce678df80f5e561886950756.png@1192w.webp" alt="img" style="zoom:50%;" />

确认号表示接收方期望下一个收到的字节的位置。接收方在收到数据后，发送确认号给发送方，告知已成功接收到的数据。发送方在接收到确认号后，会根据确认号来确定哪些数据已经被成功接收，哪些数据需要进行重传。

<img src="https://i1.hdslb.com/bfs/article/b763faa53e29c29f5e1ba575ecb579601886950756.png@1192w.webp" alt="img" style="zoom:50%;" />

在TCP三次握手时：

首先，客户端向服务器发送SYN，产生一个随机数x作为客户端的初始seq，所以此时seq=x，ack=0。

接着，服务器回复SYN和ACK，产生一个随机数y作为服务器的初始seq，ack等于上一个收到报文的seq+1，也就是客户端SYN报文的seq+1（SYN报文，TCP当做传输1个字节），即x+1。所以此时seq=y，ack=x+1。

最后，客户端回复服务器ACK，seq等于上一个自己发送报文的seq+1, 也就是SYN报文的seq+1，即x+1。而ack等于上一个收到报文的seq+1，即服务器SYN和ACK报文的seq+1（SYN报文，TCP当做传输1个字节），即y+1。所以此时seq=x+1，ack=y+1。

<img src="https://i1.hdslb.com/bfs/article/8f786ebf6d2360fe6113f5cc26c37de21886950756.png@1192w.webp" alt="img" style="zoom:50%;" />



同一个tcp连接中，在第一次握手客户端生成随机数x作为初始seq，第二次握手服务器生成随机数y作为初始seq后，后续报文的seq和ack的变化都是有规律的。

<img src="https://i1.hdslb.com/bfs/article/bedd78e0f5381be0a452e895cedbc31d1886950756.png@1192w.webp" alt="img" style="zoom:50%;" />

每个TCP报文的seq都等于上一次自己发送报文的seq加上tcp负载数据长度，如果上一次发送的报文是SYN报文或者FIN报文，则认为TCP负载数据长度为1。 而每个TCP报文的ack都等于上一次自己收到报文的seq加上tcp负载数据长度。如果上一次收到的是SYN报文或者FIN报文，则认为TCP负载数据长度为1。

<img src="https://i1.hdslb.com/bfs/article/666d477c901ac9cc485c0fd30b8db8471886950756.png@1192w.webp" alt="img" style="zoom:50%;" />

#### TCP重传机制

TCP是一种面向连接、可靠的传输层协议。为了保证数据的可靠传输，TCP采用数据包重传的机制来应对网络传输过程中可能出现的丢包、错包和乱序等问题。TCP的重传包括超时重传、快速重传、带选择确认SACK的重传和重复SACK重传四种。

<img src="https://i1.hdslb.com/bfs/article/abf95ef855bde346e065ce1a06715a3c1886950756.png@1192w.webp" alt="img" style="zoom:50%;" />



1 超时重传

当发送方发送数据包后，会启动一个重传计时器，等待接收方返回确认报文。如果在超时重传时间RTO到达之前仍未收到确认报文，发送方会认为数据包丢失，触发超时重传。超时重传的时间阈值会根据网络状况进行动态调整，应该略大于报文往返时间RTT的值。如果超时重发的数据，再次超时又需要重传的时候，TCP 的策略是超时间隔加倍。超时触发重传存在的问题是，超时周期可能相对较长。

<img src="https://i1.hdslb.com/bfs/article/0bb92f0670257da47ed89a1b53ad43771886950756.png@1192w.webp" alt="img" style="zoom:50%;" />



<img src="https://i1.hdslb.com/bfs/article/3ff16c6a13fae82c4101d12884ea0ef21886950756.png@1192w.webp" alt="img" style="zoom:50%;" />



<img src="https://i1.hdslb.com/bfs/article/cfcbca7a91e8137d447ef1120ba611de1886950756.png@1192w_894h.webp" alt="img" style="zoom:50%;" />



2 快速重传

快速重传是一种提高TCP性能的重传策略。当接收方连续收到三个相同序号的ACK确认报文时，发送方会认为对应的数据包发生了丢失。为了尽快补发丢失的数据包，发送方会立即进行重传，而不再等待重传计时器超时。这种方法可以减小因数据包丢失导致的延迟。

<img src="https://i1.hdslb.com/bfs/article/f148fdccfc27046f550e7e9b44a84f671886950756.png@1192w.webp" alt="img" style="zoom:50%;" />



3 带选择确认SACK的重传

选择性确认是一种TCP扩展，需要在TCP头部选项字段里加一个SACK的东西，它允许接收方通知发送方哪些数据包已经被成功接收，哪些数据包需要重传。SACK可以提高TCP性能，因为发送方可以更精确地知道哪些数据包需要重传，从而避免不必要的全量重传。

<img src="https://i1.hdslb.com/bfs/article/d727390edab165b4cdb3768f3c05e71e1886950756.png@1192w.webp" alt="img" style="zoom: 67%;" />



<img src="https://i1.hdslb.com/bfs/article/5dd953c61f02618f17f57607c22004671886950756.png@1192w_628h.webp" alt="img" style="zoom:50%;" />



<img src="https://i1.hdslb.com/bfs/article/f72f33d4bfa7f42dc94f87b110fed42c1886950756.png@1192w.webp" alt="img" style="zoom:50%;" />



4 重复SACK

SACK重传对于接收到重复数据段怎样运作没有明确规定，通过重复SACK重传可以让发送方知道哪些数据被重复接收了，而且明确是什么原因造成的。

<img src="https://i1.hdslb.com/bfs/article/ee794f0187b144f2280d45d09a4b5b7d1886950756.png@1192w.webp" alt="img" style="zoom:50%;" />



<img src="https://i1.hdslb.com/bfs/article/523cd0f61ff6f074f56b2560f3a9cf3c1886950756.png@1192w.webp" alt="img" style="zoom:50%;" />

#### 请说一下，TCP的流量控制是如何实现的？

流量控制是为了控制发送方发送速率，保证接收方来得及接收。TCP 利用滑动窗口来实现流量控制，接收方根据自己的处理能力，动态调整自己的可接收数据窗口大小，通过ACK报文将窗口大小告知发送方，发送方根据收到的窗口大小，调整发送数据的流量。

<img src="https://i1.hdslb.com/bfs/article/bb840bdd3fec5d78c4e1c1f6185795d81886950756.png@1192w.webp" alt="img" style="zoom:50%;" />

​     有了滑动窗口，发送方就算有海量等待发送的数据，也只能按照接收方给出的窗口大小来慢慢发。滑动窗口带来的另一个好处是，只要窗口大小允许（并且满足发送条件），发送方不必等待前面数据的ACK，也可以发送数据。接收方可以延时确认，用后面数据包的ACK，一并把前面的数据都确认了，减少了报文数量。



<img src="https://i1.hdslb.com/bfs/article/eff7a870146d5ae928b263cf0c9299c61886950756.png@1192w.webp" alt="img" style="zoom:50%;" />



<img src="https://i1.hdslb.com/bfs/article/aaa1089168bb1e8c3d3e69440cd6f7ab1886950756.png@1192w.webp" alt="img" style="zoom:50%;" />

​     发送端和接收端各自有自己的滑动窗口。发送方的滑动窗口可以分为四个部分： 1 已发送并且收到ACK确认的数据 2 已发送但未收到ACK确认的数据 3 未发送但可以发送的的数据 4 不允许被发送的数据



<img src="https://i1.hdslb.com/bfs/article/3e5a19afb6d97c7d46acbbcde44ac1ed1886950756.png@1192w.webp" alt="img" style="zoom:50%;" />



<img src="https://i1.hdslb.com/bfs/article/652296dfb423c976946f8c27e24c74021886950756.png@1192w.webp" alt="img" style="zoom:50%;" />



<img src="https://i1.hdslb.com/bfs/article/7fcf5b0ce58185a3da5a5420362e516f1886950756.png@1192w.webp" alt="img" style="zoom:50%;" />

接收方的滑动窗口可以分为三个部分： 1 接收并已确认的数据 2 等待接收且允许发送方发送的数据（接收窗口） 3 不可接收且不允许发送方发送的数据

<img src="https://i1.hdslb.com/bfs/article/196b414f17fff343ae332d78e2cf12181886950756.png@1192w.webp" alt="img" style="zoom:50%;" />

​    在数据传输过程中，接收方会不断更新窗口大小，通过TCP报文段中的窗口字段，来告知发送方当前可接收的数据量。

<img src="https://i1.hdslb.com/bfs/article/56f708d77f52958cdc560aa94e04f7331886950756.png@1192w.webp" alt="img" style="zoom:50%;" />

​     发送方根据接收方的窗口大小来调整发送的数据量，以确保发送的数据不会超过接收方的处理能力。如果接收方的窗口变小，发送方会减少发送的数据量以避免数据丢失或拥塞。当接收方的窗口变大时，发送方可以增加发送的数据量，从而提高数据传输的效率。

<img src="https://i1.hdslb.com/bfs/article/bb30dfcf04e73b949e09aa04af18e2f21886950756.png@1192w.webp" alt="img" style="zoom:50%;" />

​     需要注意的是，如果接收方太忙了，接收方有可能会关闭窗口，即设置窗口大小为0，此时发送方停止发送数据，直到窗口重新打开。为了防止重新打开窗口的报文丢失造成死锁，发送方在窗口关闭后开启，启动一个定时器，定时发送窗口探测报文，这样接收方在确认这个探测报文时，就可以给出自己现在的接收窗口大小。

<img src="https://i1.hdslb.com/bfs/article/3e3fb8c1e12e0abe15d0c93649e592541886950756.png@1192w.webp" alt="img" style="zoom:50%;" />

另外，如果接收方太忙了，来不及取走接收窗口里的数据，那么就会导致发送方的发送窗口越来越小，如果发送方每次就发送几个字节给接收方，显然是不划算的，因为这时报文中的报文头部就占了几十个字节，而真实传输的有用的数据却只有几个字节。

<img src="https://i1.hdslb.com/bfs/article/37a29e0fd03cbf21427b04ba5c33e9551886950756.png@1192w.webp" alt="img" style="zoom:50%;" />



<img src="https://i1.hdslb.com/bfs/article/b1aa1419f6ca2cacbf63f9d7ea7585d61886950756.png@1192w.webp" alt="img" style="zoom:50%;" />

这个问题叫做TCP的糊涂窗口综合症，解决糊涂窗口综合症，一方面是让接收方不通告小窗口给发送方，通常情况下，当接收窗口小于TCP最大报文段长度MSS和接收缓存一半大小中的最小值时，就会向发送方通告窗口为 0，也就阻止了发送方再发数据过来。另一方面，发送方也可以开启Nagle算法，避免发送小数据给接收方。

<img src="https://i1.hdslb.com/bfs/article/e0ab6d858aa84ca9d04c66e2a8ac42a21886950756.png@1192w.webp" alt="img" style="zoom: 67%;" />



<img src="https://i1.hdslb.com/bfs/article/204cccc86950d71fdbbdc40a64a384141886950756.png@1192w.webp" alt="img" style="zoom:67%;" />

#### 请说一下，TCP的拥塞控制是如何实现的？

流量控制是避免发送方的数据填满接收方的缓存。但计算机网络一般都处在一个共享的环境，因此也有可能会因为其他主机之间的通信使得网络拥堵。在网络出现拥堵时，如果继续发送大量数据包，可能会导致数据包丢失或者时延增大，这时TCP就会重传数据，而一重传就会导致网络的负担更重，于是会导致更大的延迟和更多的丢包。

TCP的拥塞控制机制可以根据网络链路的实时状态，自动调整发送速度，降低发送的数据量，从而避免发送方的数据填满整个网络。

TCP通过拥塞窗口来约束发送速度，拥塞窗口跟接收窗口类似，同样规定了发送方此刻能够发送出去的字节数，只不过它是通过评估网络链路的拥塞程度，并由一定的算法计算而来的。发送方的发送窗口大小由接收窗口和拥塞窗口共同决定，取拥塞窗口和接收窗口中的最小值。

TCP通过慢启动、拥塞避免、拥塞发生和快速恢复四种算法来进行拥塞控制。

1、慢启动

TCP连接刚建立时，对网络链路的运行状况一无所知，慢启动就是TCP启动后的发送速度慢慢的进行提速，便于感知网络的状况。

慢启动算法将拥塞窗口cwnd初始化为1，然后每收到一个ACK，cwnd就会加 1，假设每个报文段都会回复ACK确认，在没有丢包的情况下，第一个分组被确认后，cwnd就变成了2，接下来，TCP可以发送2个报文段。如果这2个报文段均顺利送到，可以收到2个ACK确认，cwnd增大到 4，以此类推。因此，慢启动期间cwnd是呈指数增长的。

![image-20240812231235096](https://raw.githubusercontent.com/wangbanjin1/pictures/main/image-20240812231235096.png)

2、拥塞避免

当cwnd超过慢启动门限ssthresh时就会进入拥塞避免算法。在拥塞避免阶段，TCP以更慢的速度扩张拥塞窗口，每当成功发送跟拥塞窗口大小等量的数据并收到ACK确认后，cwnd就加1。例如，假设当前拥塞窗口大小为k，这时可以发出k个报文段。当这k个报文段均发出并收到确认后，才给cwnd加1，所以拥塞避免阶段，cwnd是呈线性增长的。

3、拥塞发生 进入拥塞避免阶段后，窗口保持缓慢增长，当遇到网络拥塞发送丢包时，TCP就会进行重传。如果是发生超时重传，慢启动门限ssthresh会被设置为当前拥塞窗口值的一半，cwnd恢复为初始化值1，然后重新开始前面的慢启动过程。如果是发生快速重传（重复ACK），ssthresh和cwnd都会变为当前拥塞窗口值的一半，然后进入到快速恢复阶段。  

4、快速恢复 在快速恢复阶段，发送方会重传丢失的数据包，此时发送方已收到了3个重复的ACK，所以cwnd加3，如果再收到重复的 ACK，那么cwnd增加 1，如果收到新的ACK，表明重传的包成功了，将cwnd设置为当前的慢启动门限ssthresh值，然后再次进入到拥塞避免阶段。
